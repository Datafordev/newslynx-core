from gevent.pool import Pool

from sqlalchemy import or_

from newslynx.core import db
from newslynx import settings
from newslynx.models import (
    url_cache, Event, Thing, Tag,
    Recipe)
from newslynx.exc import RequestError
from newslynx.models.util import (
    split_meta, get_table_columns,
    fetch_by_id_or_field)
from newslynx.util import gen_uuid
from newslynx.views.util import (
    validate_thing_types, validate_event_status)
from newslynx.lib import dates
from newslynx.lib import url
from newslynx.lib import text
from newslynx.lib import html


# a pool to multithread url_cache.
url_extract_pool = Pool(settings.URL_CACHE_POOL_SIZE)


def extract_urls(obj, fields, source=None):
    """
    Extract, normalize, and dedupe urls from
    text/html fields in an object.
    """
    raw_urls = set()
    for f in fields:
        v = obj.get(f)
        if v:
            v = str(v)
            if html.is_html(v):
                for u in url.from_html(v, source):
                    raw_urls.add(u)
            else:
                for u in url.from_string(v, source):
                    raw_urls.add(u)

    clean_urls = set()
    for u in url_extract_pool.imap_unordered(url_cache.get, list(raw_urls)):
        clean_urls.add(u)
    return list(clean_urls)


def event(raw_obj,
          url_fields=['title', 'content', 'description'],
          requires=['org_id', 'title', 'content'],
          only_things=False):
    """
    Ingest an Event.
    """

    # keep track if we detected things
    has_things = False

    # get event columns.
    cols = [
        c for c in get_table_columns(Event)
        if c not in ['meta']
    ]

    # check required fields
    for k in requires:
        if k not in raw_obj:
            raise RequestError(
                "Missing '{}'. An Event Requires {}".format(k, requires))

    # get org_id
    org_id = raw_obj.get('org_id')

    # check for tags_ids + thing_ids
    tag_ids = raw_obj.pop('tag_ids', [])
    thing_ids = raw_obj.pop('thing_ids', [])

    # validate status
    if 'status' in raw_obj:
        validate_event_status(raw_obj['status'])

    # if there are tags, the status is "approved"
    if len(tag_ids):
        raw_obj['status'] = 'approved'

    # normalize the url
    if raw_obj.get('url'):
        raw_obj['url'] = url_cache.get(raw_obj['url'])

    # check dates
    if 'created' in raw_obj:
        dt = dates.parse_any(raw_obj['created'])
        if not dt:
            raise RequestError('{} is an invalid date.'
                               .format(raw_obj['created']))
        raw_obj['created'] = dt

    # sanitize text/html fields
    raw_obj['title'] = text.prepare(raw_obj['title'])


    # if there's not a recipe_id set a random source id +
    # set the recipe_id as "-1" and preface the source_id
    # as "manual"
    if 'recipe_id' not in raw_obj:
        raw_obj['source_id'] = "manual-{}".format(gen_uuid())
        raw_obj['provenance'] = 'manual'

    # if there is recipe_id, add in the
    # sous-chef-name to ensure that there
    # aren't duplicate events generated by
    # multiple child recipes of the same
    # sous-chef
    else:
        # recipe-generated events must pass in a source id
        if 'source_id' not in raw_obj:
            raise RequestError(
                'Recipe-generated events must include a source_id.')

        # fetch the associated recipe
        r = Recipe.query\
            .filter_by(id=raw_obj['recipe_id'])\
            .filter_by(org_id=org_id)\
            .first()

        if not r:
            raise RequestError('Recipe id "{recipe_id}" does not exist.'
                               .format(**raw_obj))

        # reformant source id.
        raw_obj['source_id'] = "{}-{}"\
            .format(r.sous_chef.slug, str(raw_obj['source_id']))

        # set this event as non-manual
        raw_obj['provenance'] = 'recipe'

    # split out meta fields
    raw_obj = split_meta(raw_obj, cols)

    # see if the event already exists.
    e = Event.query\
        .filter_by(org_id=org_id)\
        .filter_by(source_id=raw_obj['source_id'])\
        .first()

    # if not, create it
    if not e:

        # create event
        e = Event(**raw_obj)

    # else, update it
    else:
        for k, v in raw_obj.items():
            setattr(e, k, v)
        e.updated = dates.now()

    # extract urls asynchronously.
    urls = extract_urls(raw_obj, url_fields, source=raw_obj.get('url'))

    # detect things
    if len(urls):

        things = Thing.query\
            .filter(or_(Thing.url.in_(urls), Thing.id.in_(thing_ids)))\
            .filter(Thing.org_id == org_id)\
            .all()

        if len(things):
            has_things = True

            # upsert things.
            for t in things:
                if t.id not in e.thing_ids:
                    e.things.append(t)

    # upsert tags
    if len(tag_ids):
        for tid in tag_ids:
            tag = fetch_by_id_or_field(Tag, 'slug', tid, org_id)
            if tag:
                if tag.type != 'impact':
                    raise RequestError(
                        'Only impact tags can be associated with Events.')
                if tag.id not in e.tag_ids:
                    e.tags.append(tag)

    # dont commit event if we're only looking
    # for events that link to things
    if not has_things and only_things:
        return

    db.session.add(e)
    db.session.commit()
    return e


def thing(raw_obj,
          url_fields=['title', 'text', 'description'],
          requires=['org_id', 'url', 'type']):

    # get event columns.
    cols = [
        c for c in get_table_columns(Thing)
        if c not in ['meta']
    ]

    # check required fields
    for k in requires:
        if k not in raw_obj:
            raise RequestError(
                "Missing '{}'. An Thing Requires {}".format(k, requires))

    # get org_id
    org_id = raw_obj.get('org_id')

    # check for tags_ids + creators
    tag_ids = raw_obj.pop('tag_ids', [])
    creator_ids = raw_obj.pop('creator_ids', [])

    # normalize the url
    raw_obj['url'] = url_cache.get(raw_obj['url'])

    # check dates
    if 'created' in raw_obj:
        dt = dates.parse_iso(raw_obj['created'])
        if not dt:
            raise RequestError('{} is an invalid isodate.'
                               .format(raw_obj['created']))
        raw_obj['created'] = dt

    # remove updated
    raw_obj.pop('updated')

    # if there's not a recipe_id set the provenance
    # as "manual"
    if 'recipe_id' not in raw_obj:
        raw_obj['provenance'] = 'manual'

    # otherwise it's a recipe
    else:
        raw_obj['provenance'] = 'recipe'

    # see if the event already exists.
    e = Event.query\
        .filter_by(org_id=org_id)\
        .filter_by(source_id=raw_obj['source_id'])\
        .first()

    # if not, create it
    if not e:

        # create event
        e = Event(**raw_obj)

    # else, update it
    else:
        for k, v in raw_obj.items():
            setattr(e, k, v)
        e.updated = dates.now()

    # extract urls asynchronously.
    urls = extract_urls(raw_obj, fields=url_fields)

    # detect things
    if len(urls):

        things = Thing.query\
            .filter(or_(Thing.url.in_(urls), Thing.id.in_(thing_ids)))\
            .filter(Thing.org_id == org_id)\
            .all()

        if len(things):
            has_things = True

            # upsert things.
            for t in things:
                if t.id not in e.thing_ids:
                    e.things.append(t)

    # upsert tags
    if len(tag_ids):
        for tid in tag_ids:
            tag = fetch_by_id_or_field(Tag, 'slug', tid, org_id)
            if tag:
                if tag.type != 'impact':
                    raise RequestError(
                        'Only impact tags can be associated with Events.')
                if tag.id not in e.tag_ids:
                    e.tags.append(tag)

    # dont commit event if we're only looking
    # for events that link to things
    if not has_things and only_things:
        return

    db.session.add(e)
    db.session.commit()
    return e
